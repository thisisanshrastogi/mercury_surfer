{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a87a0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import gensim\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "from tqdm import tqdm\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26bc25de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47837/47837 [00:00<00:00, 704576.38it/s]\n",
      "100%|██████████| 47837/47837 [00:00<00:00, 116005.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['trouble', 'log', 'give', 'something_go', 'wrong', 'restart_phone', 'log', 'also', 'cache_clear', 'old', 'review', 'app', 'great', 'try', 'use', 'tablet', 'deposit', 'could', 'take_picture', 'download', 'app', 'phone', 'work', 'great', 'take', 'minute', 'find', 'way', 'around', 'app', 'good'], ['option', 'temporarily', 'lock', 'debit_card'], ['app', 'great', 'crash', 'time', 'every', 'log', 'issue', 'since', 'update', 'every_time', 'attempt', 'log', 'app', 'say', 'log', 'due', 'activity', 'minute', 'please', 'log', 'something', 'similar', 'often', 'get', 'message', 'additionally', 'also', 'get', 'messase', 'say', 'app', 'constantly_crash', 'need', 'close', 'please_fix', 'asap', 'know', 'reason', 'change', 'bank'], ['terrible', 'app', 'terrible', 'customer_service', 'stay', 'far', 'away', 'company', 'possible'], ['suck']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"./sentiment_classified.csv\")\n",
    "\n",
    "def clean_more(text):\n",
    "    text = text.lower()\n",
    "    return text.split()\n",
    "\n",
    "df = df[df['Stars'] !=5].reset_index(drop=True)\n",
    "df['Cleaned Reviews'] = df['Cleaned Reviews'].fillna('')\n",
    "df['Cleaned Reviews'] = df['Cleaned Reviews']\n",
    "df['tokens'] = df[\"Cleaned Reviews\"].progress_apply(clean_more)\n",
    "\n",
    "bigram_phrases = Phrases(df['tokens'],min_count=5,threshold=5)\n",
    "\n",
    "bigram_mod = Phraser(bigram_phrases)\n",
    "\n",
    "df['bigram_tokens'] = df['tokens'].progress_apply(lambda doc: bigram_mod[doc])\n",
    "\n",
    "print(df['bigram_tokens'].sample(5).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d825b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dictionary = corpora.Dictionary(df['bigram_tokens'])\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=5000)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in df['bigram_tokens']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d7ca17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, start=5, limit=20, step=1):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit + 1, step):\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num_topics,\n",
    "            passes=15,\n",
    "            workers=6,\n",
    "            random_state=42\n",
    "        )\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_score = coherencemodel.get_coherence()\n",
    "        coherence_values.append(coherence_score)\n",
    "        print(f'Number of topics: {num_topics}, Coherence Score: {coherence_score:.4f}')\n",
    "    return model_list, coherence_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b4efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 5, Coherence Score: 0.5663\n",
      "Number of topics: 6, Coherence Score: 0.5703\n",
      "Number of topics: 7, Coherence Score: 0.5674\n",
      "Number of topics: 8, Coherence Score: 0.5615\n",
      "Number of topics: 9, Coherence Score: 0.5760\n",
      "Number of topics: 10, Coherence Score: 0.5649\n",
      "Number of topics: 11, Coherence Score: 0.5628\n",
      "Number of topics: 12, Coherence Score: 0.5771\n",
      "Number of topics: 13, Coherence Score: 0.5617\n",
      "Number of topics: 14, Coherence Score: 0.5604\n",
      "Number of topics: 15, Coherence Score: 0.5562\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary, corpus, df['bigram_tokens'], start=5, limit=15, step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a953bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model_list[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "914c0a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_data = []\n",
    "for num_topics, model in zip(range(5, 16), model_list):\n",
    "    for idx, topic in model.print_topics(num_topics=num_topics, num_words=8):\n",
    "        topics_data.append({\n",
    "            \"Model_Num_Topics\": num_topics,\n",
    "            \"Topic_Index\": idx,\n",
    "            \"Topic_Terms\": topic\n",
    "        })\n",
    "\n",
    "topics_df = pd.DataFrame(topics_data)\n",
    "topics_df.to_csv(\"LDA_topics_by_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "539aebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a document-topic distribution matrix\n",
    "doc_topic_dists = []\n",
    "\n",
    "for doc in corpus:\n",
    "    topic_probs = best_model.get_document_topics(doc, minimum_probability=0)\n",
    "    doc_topic_dists.append([prob for _, prob in topic_probs])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_topic_dist = pd.DataFrame(doc_topic_dists)\n",
    "df_topic_dist.columns = [f'Topic {i}' for i in range(df_topic_dist.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d6cee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Count per Dominant Topic:\n",
      "Topic 0     4468\n",
      "Topic 1     3803\n",
      "Topic 10    2290\n",
      "Topic 11    3407\n",
      "Topic 12    2908\n",
      "Topic 2     5389\n",
      "Topic 3     3833\n",
      "Topic 4     6302\n",
      "Topic 5     2771\n",
      "Topic 6     3252\n",
      "Topic 7     2234\n",
      "Topic 8     3446\n",
      "Topic 9     3734\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find dominant topic in each document\n",
    "dominant_topics = df_topic_dist.idxmax(axis=1)\n",
    "dominant_topic_counts = dominant_topics.value_counts().sort_index()\n",
    "\n",
    "# Print count of documents dominated by each topic\n",
    "print(\"Document Count per Dominant Topic:\")\n",
    "print(dominant_topic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f000313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Share of Each Topic:\n",
      "Topic 4     0.111135\n",
      "Topic 2     0.102087\n",
      "Topic 3     0.081357\n",
      "Topic 9     0.080859\n",
      "Topic 0     0.080770\n",
      "Topic 1     0.080437\n",
      "Topic 11    0.075653\n",
      "Topic 8     0.073461\n",
      "Topic 6     0.069728\n",
      "Topic 12    0.064968\n",
      "Topic 5     0.063921\n",
      "Topic 7     0.058168\n",
      "Topic 10    0.057457\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "topic_shares = df_topic_dist.mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nAverage Share of Each Topic:\")\n",
    "print(topic_shares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4334c05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Summary:\n",
      "          Dominant Doc Count  Average Share\n",
      "Topic 4                 6302       0.111135\n",
      "Topic 2                 5389       0.102087\n",
      "Topic 0                 4468       0.080770\n",
      "Topic 3                 3833       0.081357\n",
      "Topic 1                 3803       0.080437\n",
      "Topic 9                 3734       0.080859\n",
      "Topic 8                 3446       0.073461\n",
      "Topic 11                3407       0.075653\n",
      "Topic 6                 3252       0.069728\n",
      "Topic 12                2908       0.064968\n",
      "Topic 5                 2771       0.063921\n",
      "Topic 10                2290       0.057457\n",
      "Topic 7                 2234       0.058168\n"
     ]
    }
   ],
   "source": [
    "summary_df = pd.DataFrame({\n",
    "    \"Dominant Doc Count\": dominant_topic_counts,\n",
    "    \"Average Share\": df_topic_dist.mean()\n",
    "}).fillna(0)\n",
    "\n",
    "summary_df = summary_df.sort_values(\"Dominant Doc Count\", ascending=False)\n",
    "print(\"\\nTopic Summary:\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "462878ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.to_csv(\"LDA_quantitative.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b466f8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Values:\n",
      "Number of Topics: 5, Coherence Score: 0.5663\n",
      "Number of Topics: 6, Coherence Score: 0.5703\n",
      "Number of Topics: 7, Coherence Score: 0.5674\n",
      "Number of Topics: 8, Coherence Score: 0.5615\n",
      "Number of Topics: 9, Coherence Score: 0.5760\n",
      "Number of Topics: 10, Coherence Score: 0.5649\n",
      "Number of Topics: 11, Coherence Score: 0.5628\n",
      "Number of Topics: 12, Coherence Score: 0.5771\n",
      "Number of Topics: 13, Coherence Score: 0.5617\n",
      "Number of Topics: 14, Coherence Score: 0.5604\n",
      "Number of Topics: 15, Coherence Score: 0.5562\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCoherence Values:\")\n",
    "for num_topics, coherence in zip(range(5, 16), coherence_values):\n",
    "    print(f\"Number of Topics: {num_topics}, Coherence Score: {coherence:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

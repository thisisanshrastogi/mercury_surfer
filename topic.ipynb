{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a87a0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import gensim\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "from tqdm import tqdm\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26bc25de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4691/4691 [00:00<00:00, 66772.37it/s]\n",
      "100%|██████████| 4691/4691 [00:00<00:00, 137125.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['worth', 'pay', 'many', 'glitch', 'constantly', 'lose_connection', 'instead', 'fix', 'force', 'delete', 'everything', 'reconnect', 'account', 'big', 'deal', 'expect', 'manually', 'create_rule', 'support', 'terribly', 'slow', 'useless', 'every', 'exist', 'retail', 'could', 'add', 'datum', 'minute', 'would', 'know', 'michael', 'shop', 'categorize', 'properly', 'currently', 'pay', 'teach', 'bad', 'program', 'basic', 'datum', 'never'], ['happy', 'although', 'immediately', 'get', 'refund', 'monthly_fee', 'apparently', 'use', 'approve', 'least', 'advance', 'usual', 'bank', 'glitch', 'allow', 'advance', 'figure', 'really', 'need', 'right'], ['respond', 'robot', 'robot', 'disrespectful', 'robot_answer', 'question', 'feel_like', 'answer', 'feel', 'handle', 'cleo', 'direct', 'contact', 'put', 'money', 'prepared', 'able', 'withdraw', 'atm', 'put', 'platform', 'transfer_money', 'go', 'tell', 'western', 'union', 'accept', 'lot', 'place'], ['disappointing'], ['app', 'work_properly', 'send', 'login', 'code_email', 'also', 'get', 'stuck', 'determine', 'eligibility', 'repay', 'guy', 'today', 'totally', 'guy', 'play', 'money', 'point']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"./sentiment_classified_2.csv\")\n",
    "\n",
    "def clean_more(text):\n",
    "    text = text.lower()\n",
    "    return text.split()\n",
    "\n",
    "df = df[df['Stars'] !=5].reset_index(drop=True)\n",
    "df['Cleaned Reviews'] = df['Cleaned Reviews'].fillna('')\n",
    "df['Cleaned Reviews'] = df['Cleaned Reviews']\n",
    "df['tokens'] = df[\"Cleaned Reviews\"].progress_apply(clean_more)\n",
    "\n",
    "bigram_phrases = Phrases(df['tokens'],min_count=5,threshold=5)\n",
    "\n",
    "bigram_mod = Phraser(bigram_phrases)\n",
    "\n",
    "df['bigram_tokens'] = df['tokens'].progress_apply(lambda doc: bigram_mod[doc])\n",
    "\n",
    "print(df['bigram_tokens'].sample(5).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d825b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dictionary = corpora.Dictionary(df['bigram_tokens'])\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=5000)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in df['bigram_tokens']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d7ca17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, start=5, limit=20, step=1):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit + 1, step):\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num_topics,\n",
    "            passes=15,\n",
    "            workers=6,\n",
    "            random_state=42\n",
    "        )\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_score = coherencemodel.get_coherence()\n",
    "        coherence_values.append(coherence_score)\n",
    "        print(f'Number of topics: {num_topics}, Coherence Score: {coherence_score:.4f}')\n",
    "    return model_list, coherence_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b4efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 5, Coherence Score: 0.5560\n",
      "Number of topics: 6, Coherence Score: 0.5373\n",
      "Number of topics: 7, Coherence Score: 0.5308\n",
      "Number of topics: 8, Coherence Score: 0.5105\n",
      "Number of topics: 9, Coherence Score: 0.5067\n",
      "Number of topics: 10, Coherence Score: 0.4936\n",
      "Number of topics: 11, Coherence Score: 0.5115\n",
      "Number of topics: 12, Coherence Score: 0.4716\n",
      "Number of topics: 13, Coherence Score: 0.5038\n",
      "Number of topics: 14, Coherence Score: 0.4947\n",
      "Number of topics: 15, Coherence Score: 0.5009\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary, corpus, df['bigram_tokens'], start=5, limit=15, step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a953bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model_list[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "914c0a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_data = []\n",
    "for num_topics, model in zip(range(5, 16), model_list):\n",
    "    for idx, topic in model.print_topics(num_topics=num_topics, num_words=8):\n",
    "        topics_data.append({\n",
    "            \"Model_Num_Topics\": num_topics,\n",
    "            \"Topic_Index\": idx,\n",
    "            \"Topic_Terms\": topic\n",
    "        })\n",
    "\n",
    "topics_df = pd.DataFrame(topics_data)\n",
    "topics_df.to_csv(\"LDA_topics_by_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "539aebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a document-topic distribution matrix\n",
    "doc_topic_dists = []\n",
    "\n",
    "for doc in corpus:\n",
    "    topic_probs = best_model.get_document_topics(doc, minimum_probability=0)\n",
    "    doc_topic_dists.append([prob for _, prob in topic_probs])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_topic_dist = pd.DataFrame(doc_topic_dists)\n",
    "df_topic_dist.columns = [f'Topic {i}' for i in range(df_topic_dist.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d6cee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Count per Dominant Topic:\n",
      "Topic 0     444\n",
      "Topic 1     271\n",
      "Topic 10    405\n",
      "Topic 11    276\n",
      "Topic 12    491\n",
      "Topic 2     399\n",
      "Topic 3     222\n",
      "Topic 4     310\n",
      "Topic 5     360\n",
      "Topic 6     446\n",
      "Topic 7     366\n",
      "Topic 8     388\n",
      "Topic 9     313\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find dominant topic in each document\n",
    "dominant_topics = df_topic_dist.idxmax(axis=1)\n",
    "dominant_topic_counts = dominant_topics.value_counts().sort_index()\n",
    "\n",
    "# Print count of documents dominated by each topic\n",
    "print(\"Document Count per Dominant Topic:\")\n",
    "print(dominant_topic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f000313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Share of Each Topic:\n",
      "Topic 12    0.101516\n",
      "Topic 6     0.097029\n",
      "Topic 8     0.084655\n",
      "Topic 10    0.081220\n",
      "Topic 0     0.080696\n",
      "Topic 2     0.079266\n",
      "Topic 7     0.079252\n",
      "Topic 5     0.073908\n",
      "Topic 4     0.071641\n",
      "Topic 9     0.069965\n",
      "Topic 1     0.062794\n",
      "Topic 11    0.061595\n",
      "Topic 3     0.056461\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "topic_shares = df_topic_dist.mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nAverage Share of Each Topic:\")\n",
    "print(topic_shares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4334c05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Summary:\n",
      "          Dominant Doc Count  Average Share\n",
      "Topic 12                 491       0.101516\n",
      "Topic 6                  446       0.097029\n",
      "Topic 0                  444       0.080696\n",
      "Topic 10                 405       0.081220\n",
      "Topic 2                  399       0.079266\n",
      "Topic 8                  388       0.084655\n",
      "Topic 7                  366       0.079252\n",
      "Topic 5                  360       0.073908\n",
      "Topic 9                  313       0.069965\n",
      "Topic 4                  310       0.071641\n",
      "Topic 11                 276       0.061595\n",
      "Topic 1                  271       0.062794\n",
      "Topic 3                  222       0.056461\n"
     ]
    }
   ],
   "source": [
    "summary_df = pd.DataFrame({\n",
    "    \"Dominant Doc Count\": dominant_topic_counts,\n",
    "    \"Average Share\": df_topic_dist.mean()\n",
    "}).fillna(0)\n",
    "\n",
    "summary_df = summary_df.sort_values(\"Dominant Doc Count\", ascending=False)\n",
    "print(\"\\nTopic Summary:\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "462878ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.to_csv(\"LDA_quantitative.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b466f8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Values:\n",
      "Number of Topics: 5, Coherence Score: 0.5560\n",
      "Number of Topics: 6, Coherence Score: 0.5373\n",
      "Number of Topics: 7, Coherence Score: 0.5308\n",
      "Number of Topics: 8, Coherence Score: 0.5105\n",
      "Number of Topics: 9, Coherence Score: 0.5067\n",
      "Number of Topics: 10, Coherence Score: 0.4936\n",
      "Number of Topics: 11, Coherence Score: 0.5115\n",
      "Number of Topics: 12, Coherence Score: 0.4716\n",
      "Number of Topics: 13, Coherence Score: 0.5038\n",
      "Number of Topics: 14, Coherence Score: 0.4947\n",
      "Number of Topics: 15, Coherence Score: 0.5009\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCoherence Values:\")\n",
    "for num_topics, coherence in zip(range(5, 16), coherence_values):\n",
    "    print(f\"Number of Topics: {num_topics}, Coherence Score: {coherence:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
